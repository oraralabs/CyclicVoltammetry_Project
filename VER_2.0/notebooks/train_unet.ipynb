{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VER 2.0 - Peak Deconvolution Model Training\n",
                "\n",
                "**Objective:** Train a 1D U-Net to predict Gaussian peak parameters from summed signals.\n",
                "\n",
                "**Input:** Summed signal (500 points)  \n",
                "**Output:** Peak heatmap (probability of peak centers)\n",
                "\n",
                "---\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive to access training data\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Set path to your training data (in Colab Notebooks folder)\n",
                "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/training_data.npz'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')\n",
                "print(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the generated dataset\n",
                "data = np.load(DATA_PATH)\n",
                "signals = data['signals']\n",
                "heatmaps = data['heatmaps']\n",
                "voltage_grid = data['voltage_grid']\n",
                "\n",
                "print(f'Signals shape: {signals.shape}')\n",
                "print(f'Heatmaps shape: {heatmaps.shape}')\n",
                "print(f'Voltage grid: {voltage_grid.min():.2f} to {voltage_grid.max():.2f} V')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize signals (important for neural net training)\n",
                "signal_mean = signals.mean()\n",
                "signal_std = signals.std()\n",
                "signals_norm = (signals - signal_mean) / signal_std\n",
                "\n",
                "print(f'Normalized signals: mean={signals_norm.mean():.4f}, std={signals_norm.std():.4f}')\n",
                "\n",
                "# Reshape for Conv1D: (samples, timesteps, features)\n",
                "X = signals_norm.reshape(-1, 500, 1)\n",
                "Y = heatmaps.reshape(-1, 500, 1)\n",
                "\n",
                "# Train/validation split\n",
                "split_idx = int(0.9 * len(X))\n",
                "X_train, X_val = X[:split_idx], X[split_idx:]\n",
                "Y_train, Y_val = Y[:split_idx], Y[split_idx:]\n",
                "\n",
                "print(f'Training: {len(X_train)}, Validation: {len(X_val)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build 1D U-Net Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_unet_1d(input_shape=(500, 1)):\n",
                "    \"\"\"\n",
                "    1D U-Net for peak heatmap prediction.\n",
                "    Encoder-decoder with skip connections.\n",
                "    \"\"\"\n",
                "    inputs = keras.Input(shape=input_shape)\n",
                "    \n",
                "    # --- ENCODER ---\n",
                "    # Block 1\n",
                "    c1 = layers.Conv1D(32, 7, padding='same', activation='relu')(inputs)\n",
                "    c1 = layers.Conv1D(32, 7, padding='same', activation='relu')(c1)\n",
                "    p1 = layers.MaxPooling1D(2)(c1)  # 250\n",
                "    \n",
                "    # Block 2\n",
                "    c2 = layers.Conv1D(64, 5, padding='same', activation='relu')(p1)\n",
                "    c2 = layers.Conv1D(64, 5, padding='same', activation='relu')(c2)\n",
                "    p2 = layers.MaxPooling1D(2)(c2)  # 125\n",
                "    \n",
                "    # Block 3\n",
                "    c3 = layers.Conv1D(128, 3, padding='same', activation='relu')(p2)\n",
                "    c3 = layers.Conv1D(128, 3, padding='same', activation='relu')(c3)\n",
                "    p3 = layers.MaxPooling1D(5)(c3)  # 25\n",
                "    \n",
                "    # --- BOTTLENECK ---\n",
                "    c4 = layers.Conv1D(256, 3, padding='same', activation='relu')(p3)\n",
                "    c4 = layers.Conv1D(256, 3, padding='same', activation='relu')(c4)\n",
                "    \n",
                "    # --- DECODER ---\n",
                "    # Block 3\n",
                "    u3 = layers.UpSampling1D(5)(c4)  # 125\n",
                "    u3 = layers.Concatenate()([u3, c3])\n",
                "    d3 = layers.Conv1D(128, 3, padding='same', activation='relu')(u3)\n",
                "    d3 = layers.Conv1D(128, 3, padding='same', activation='relu')(d3)\n",
                "    \n",
                "    # Block 2\n",
                "    u2 = layers.UpSampling1D(2)(d3)  # 250\n",
                "    u2 = layers.Concatenate()([u2, c2])\n",
                "    d2 = layers.Conv1D(64, 5, padding='same', activation='relu')(u2)\n",
                "    d2 = layers.Conv1D(64, 5, padding='same', activation='relu')(d2)\n",
                "    \n",
                "    # Block 1\n",
                "    u1 = layers.UpSampling1D(2)(d2)  # 500\n",
                "    u1 = layers.Concatenate()([u1, c1])\n",
                "    d1 = layers.Conv1D(32, 7, padding='same', activation='relu')(u1)\n",
                "    d1 = layers.Conv1D(32, 7, padding='same', activation='relu')(d1)\n",
                "    \n",
                "    # Output: peak probability heatmap\n",
                "    outputs = layers.Conv1D(1, 1, activation='sigmoid')(d1)\n",
                "    \n",
                "    model = keras.Model(inputs, outputs, name='UNet1D_PeakDetector')\n",
                "    return model\n",
                "\n",
                "model = build_unet_1d()\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile model\n",
                "model.compile(\n",
                "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['mae']\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Callbacks\n",
                "callbacks = [\n",
                "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
                "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
                "]\n",
                "\n",
                "# Train\n",
                "history = model.fit(\n",
                "    X_train, Y_train,\n",
                "    validation_data=(X_val, Y_val),\n",
                "    epochs=50,\n",
                "    batch_size=64,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].plot(history.history['loss'], label='Train')\n",
                "axes[0].plot(history.history['val_loss'], label='Val')\n",
                "axes[0].set_title('Loss')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].plot(history.history['mae'], label='Train')\n",
                "axes[1].plot(history.history['val_mae'], label='Val')\n",
                "axes[1].set_title('MAE')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate on Test Samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on validation samples\n",
                "fig, axes = plt.subplots(5, 3, figsize=(15, 16))\n",
                "\n",
                "for i in range(5):\n",
                "    idx = np.random.randint(len(X_val))\n",
                "    \n",
                "    signal = X_val[idx:idx+1]\n",
                "    true_heatmap = Y_val[idx].squeeze()\n",
                "    pred_heatmap = model.predict(signal, verbose=0).squeeze()\n",
                "    \n",
                "    # Denormalize signal for display\n",
                "    signal_display = signal.squeeze() * signal_std + signal_mean\n",
                "    \n",
                "    # Plot signal\n",
                "    axes[i, 0].plot(voltage_grid, signal_display, 'b-')\n",
                "    axes[i, 0].set_title('Input Signal')\n",
                "    axes[i, 0].set_xlabel('V')\n",
                "    axes[i, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Plot true heatmap\n",
                "    axes[i, 1].fill_between(voltage_grid, true_heatmap, alpha=0.5, color='green')\n",
                "    axes[i, 1].set_title('True Heatmap')\n",
                "    axes[i, 1].set_ylim(0, 1.1)\n",
                "    axes[i, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Plot predicted heatmap\n",
                "    axes[i, 2].fill_between(voltage_grid, pred_heatmap, alpha=0.5, color='orange')\n",
                "    axes[i, 2].set_title('Predicted Heatmap')\n",
                "    axes[i, 2].set_ylim(0, 1.1)\n",
                "    axes[i, 2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save normalization constants (needed for inference)\n",
                "np.savez('normalization_params.npz', \n",
                "         signal_mean=signal_mean, \n",
                "         signal_std=signal_std,\n",
                "         voltage_grid=voltage_grid)\n",
                "\n",
                "# Save model\n",
                "model.save('peak_detector.keras')\n",
                "print('Model saved!')\n",
                "\n",
                "# Download files\n",
                "from google.colab import files\n",
                "files.download('peak_detector.keras')\n",
                "files.download('normalization_params.npz')"
            ]
        }
    ]
}